import os
from dotenv import load_dotenv
from pathlib import Path
import logging
from fuzzywuzzy import fuzz
from langchain_core.prompts import ChatPromptTemplate, FewShotPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from config import answer_examples

# .env ë¡œë“œ (íŒŒì¼ ê²½ë¡œ ëª…ì‹œ)
dotenv_path = Path(__file__).resolve().parent / ".env"
load_dotenv(dotenv_path)

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)

# ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """ì„¸ì…˜ IDë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬"""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

def get_retriever():
    """Pinecone ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    embedding = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=api_key
    )
    
    index_name = "privacychat"
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)
    return database.as_retriever(search_kwargs={"k": 5})

def get_llm(model: str = "gpt-4o") -> ChatOpenAI:
    """GPT-4o ëª¨ë¸ì„ ì„¤ì •"""
    return ChatOpenAI(model=model, streaming=True)

### `get_rag_chain()` ì¶”ê°€ (ì˜¤ë¥˜ í•´ê²°)
def get_rag_chain() -> RunnableWithMessageHistory:
    """RAG ì²´ì¸ ìƒì„±"""
    llm = get_llm()
    retriever = get_retriever()

    system_prompt = (
        "ë‹¹ì‹ ì€ ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµì˜ ê°œì¸ì •ë³´ ë³´í˜¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. "
        "ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”. "
        "ì¶œì²˜ëŠ” '(ì¶œì²˜: ë¬¸ì„œëª…: [ë¬¸ì„œëª…], í˜ì´ì§€: [í˜ì´ì§€])' í˜•ì‹ìœ¼ë¡œ í¬í•¨í•˜ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”. "
        "ë§Œì•½ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë¶€ì¡±í•˜ë©´ 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì‹œê² ì–´ìš”?'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n\n"
        "{context}"
    )

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    return RunnableWithMessageHistory(
        rag_chain, get_session_history,
        input_messages_key="input", history_messages_key="chat_history", output_messages_key="answer",
    )

### ì§ˆë¬¸ ì •ê·œí™” ê°œì„ 
def normalize_query(user_message: str) -> str:
    """êµ¬ì–´ì²´ ë° ë‹¤ì–‘í•œ í‘œí˜„ì„ í‘œì¤€í™”í•˜ì—¬ ë³€í™˜"""
    replacements = {
        "ì„œìš¸ê³¼ê¸°ëŒ€": "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ",
        "ê³¼ê¸°ëŒ€": "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ",
        "í•™êµ": "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ",
        "ë³´ìœ  ê¸°ê°„": "ë³´ì¡´ ê¸°ê°„",
        "ì €ì¥ ê¸°ê°„": "ë³´ì¡´ ê¸°ê°„",
        "ê°œì¸ì •ë³´": "ê°œì¸ì •ë³´ ìˆ˜ì§‘ í•­ëª©",
        "ë‚´ ì •ë³´": "ê°œì¸ì •ë³´",
        "í•™êµì—ì„œ ë‚´ ê°œì¸ì •ë³´": "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ ê°œì¸ì •ë³´",
        "ì¡¸ì—…í•˜ë©´ ë‚´ ê°œì¸ì •ë³´": "ì¡¸ì—… í›„ ê°œì¸ì •ë³´",
        "ì¡¸ì—…í•˜ê³  ë‚˜ë©´ ê°œì¸ì •ë³´": "ì¡¸ì—… í›„ ê°œì¸ì •ë³´",
        "í•™êµ ì¡¸ì—…í•˜ë©´ ë‚´ ê°œì¸ì •ë³´": "ì¡¸ì—… í›„ ê°œì¸ì •ë³´",
        "í•™êµ ì¡¸ì—… í›„ ë‚´ ì •ë³´": "ì¡¸ì—… í›„ ê°œì¸ì •ë³´",
        "í•™êµ ì¡¸ì—… í›„ ë‚´ ê°œì¸ì •ë³´ëŠ”": "ì¡¸ì—… í›„ ê°œì¸ì •ë³´",
        "ì¡¸ì—…í•˜ë©´ ì •ë³´ ì‚­ì œ?": "ì¡¸ì—… í›„ ê°œì¸ì •ë³´ ì²˜ë¦¬",
        "ì¡¸ì—…í•˜ë©´ ê°œì¸ ì •ë³´ëŠ” ì–´ë–»ê²Œ ë¼?": "ì¡¸ì—… í›„ ê°œì¸ì •ë³´ ì²˜ë¦¬",
        "ì¡¸ì—… í›„ ë‚´ ì •ë³´ ì—†ì–´ì§€ë‚˜ìš”?": "ì¡¸ì—… í›„ ê°œì¸ì •ë³´ ì‚­ì œ ì—¬ë¶€"
    }
    
    for key, value in replacements.items():
        user_message = user_message.replace(key, value)

    return user_message

### ì§ˆì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
def classify_query(user_message: str) -> str:
    """ì§ˆë¬¸ì„ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜"""
    llm = get_llm()
    classification_prompt = f"""
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”: 
    1. ê°œì¸ì •ë³´ ìˆ˜ì§‘ í•­ëª©
    2. ê°œì¸ì •ë³´ ë³´ì¡´ ê¸°ê°„
    3. ê°œì¸ì •ë³´ ìš´ì˜ ëª©ì 
    4. ê¸°íƒ€

    ì§ˆë¬¸: "{user_message}"
    ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    """

    response = llm.invoke(classification_prompt)

    # ğŸ” íƒ€ì… ë° ì‘ë‹µ ì§ì ‘ í™•ì¸
    print("ì‘ë‹µ íƒ€ì…:", type(response))
    print("ì‘ë‹µ ë‚´ìš©:", response)

    return "4"  # ì¼ë‹¨ ê¸°ë³¸ê°’ ë°˜í™˜í•´ì„œ ì—ëŸ¬ í”¼í•˜ê¸°

### ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ìœ ì‚¬ ì§ˆë¬¸ ì¶”ì²œ
def find_similar_questions(query, retriever, top_n=3):
    """ì…ë ¥ëœ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ìŒ"""
    docs = retriever.invoke(query)
    best_docs = sorted(docs, key=lambda d: fuzz.ratio(query, d.page_content), reverse=True)[:top_n]
    return best_docs

### ë©”ì¸ í•¨ìˆ˜ (ì˜¤ë¥˜ ìˆ˜ì •)
from config import answer_examples

def get_ai_response(user_message: str, session_id: str) -> str:
    """ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•œ í›„ ì ì ˆí•œ ê²€ìƒ‰ ë°©ì‹ì„ ì„ íƒ"""
    try:
        # (1) ì§ˆë¬¸ ì •ê·œí™”
        normalized_query = normalize_query(user_message)

        # (2) ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        category = classify_query(normalized_query)

        # (3) ë¬¸ì„œ ê²€ìƒ‰
        retriever = get_retriever()
        retrieved_docs = retriever.invoke(normalized_query)

        # (4) ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Few-shot ë°ì´í„°ì—ì„œ ìœ ì‚¬ ì§ˆë¬¸ ì°¾ê¸°
        if not retrieved_docs:
            best_match = None
            best_score = 0

            for example in answer_examples:
                similarity = fuzz.ratio(normalized_query, example["input"])
                if similarity > best_score and similarity > 80:  # 80% ì´ìƒ ìœ ì‚¬í•œ ì§ˆë¬¸ ì°¾ê¸°
                    best_score = similarity
                    best_match = example["answer"]

            if best_match:
                return best_match  # ìœ ì‚¬í•œ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ í•´ë‹¹ ë‹µë³€ ì œê³µ

            # (5) ê·¸ë˜ë„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ìœ ì‚¬ ì§ˆë¬¸ ì¶”ì²œ
            similar_docs = find_similar_questions(normalized_query, retriever)
            if similar_docs:
                suggestions = "\n".join([f"- {doc.page_content[:50]}..." for doc in similar_docs])
                return f"â“ ì°¾ìœ¼ì‹œëŠ” ì •ë³´ê°€ ì—†ì–´ìš”. í˜¹ì‹œ ì´ëŸ° ì§ˆë¬¸ì„ í•˜ì‹œë ¤ í–ˆë‚˜ìš”?\n\n{suggestions}"
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # (6) ë‹µë³€ ìƒì„±
        rag_chain = get_rag_chain()
        ai_response = rag_chain.invoke(
            {"input": normalized_query},
            config={"configurable": {"session_id": session_id}},
        )

        # ì•ˆì „í•˜ê²Œ ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
        if isinstance(ai_response, dict):
            answer = ai_response.get("answer", "")
        elif hasattr(ai_response, "content"):
            answer = ai_response.content
        else:
            answer = str(ai_response)

        return answer.strip()

    except Exception as e:
        logging.error(f"âŒ AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
