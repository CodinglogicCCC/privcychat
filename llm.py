import os
from dotenv import load_dotenv
from pathlib import Path
import logging
from fuzzywuzzy import fuzz
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from config import answer_examples

# .env ë¡œë“œ (íŒŒì¼ ê²½ë¡œ ëª…ì‹œ)
dotenv_path = Path(__file__).resolve().parent / ".env"
load_dotenv(dotenv_path)

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)

# ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

def get_retriever():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    embedding = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=api_key
    )
    index_name = "privacychat"
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)
    return database.as_retriever(search_kwargs={"k": 15})

def get_llm(model: str = "gpt-4o") -> ChatOpenAI:
    return ChatOpenAI(model=model, streaming=True)

def get_rag_chain() -> RunnableWithMessageHistory:
    llm = get_llm()
    retriever = get_retriever()

    system_prompt = (
        "ë‹¹ì‹ ì€ ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµì˜ ê°œì¸ì •ë³´ ë³´í˜¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. "
        "ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”. "
        "ê°€ëŠ¥í•˜ë‹¤ë©´ ì œê³µ í•­ëª©, ì œê³µ ëŒ€ìƒ ê¸°ê´€, ë³´ìœ  ê¸°ê°„, ë²•ì  ê·¼ê±° ë“±ì„ êµ¬ì²´ì ìœ¼ë¡œ í¬í•¨í•˜ì„¸ìš”. "
        "ì¶œì²˜ëŠ” '(ì¶œì²˜: ë¬¸ì„œëª…: [ë¬¸ì„œëª…], í˜ì´ì§€: [í˜ì´ì§€])' í˜•ì‹ìœ¼ë¡œ í¬í•¨í•˜ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”. "
        "ë¬¸ì„œì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ëª…ì‹œë˜ì§€ ì•Šì•˜ë”ë¼ë„, ìœ ì‚¬í•œ í•­ëª©ì´ë‚˜ ì¼ë°˜ì ì¸ ì›ì¹™ì´ ìˆë‹¤ë©´ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. "
        "ì •ë³´ê°€ ë¶€ì¡±í•˜ë”ë¼ë„ ì§ˆë¬¸ìì˜ ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ìµœì†Œí•œì˜ ì›ì¹™ì´ë‚˜ ìœ ì‚¬ ë¬¸ë§¥ì„ ê·¼ê±°ë¡œ ì„¤ëª…í•˜ì„¸ìš”. "
        "ë‹¨, ì „í˜€ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ë§Œ 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì‹œê² ì–´ìš”?'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”."
        "\n\n{context}"
    )

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    return RunnableWithMessageHistory(
        rag_chain, get_session_history,
        input_messages_key="input", history_messages_key="chat_history", output_messages_key="answer",
    )

def get_ai_response(user_message: str, session_id: str) -> str:
    try:
        normalized_query = user_message.strip()
        retriever = get_retriever()
        retrieved_docs = retriever.invoke(normalized_query)

        print("ğŸ” [ê²€ìƒ‰ëœ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°]")
        if retrieved_docs:
            for doc in retrieved_docs:
                print("-", doc.page_content[:200].replace("\n", " "), "...")

        rag_chain = get_rag_chain()
        ai_response = rag_chain.invoke(
            {"input": normalized_query},
            config={"configurable": {"session_id": session_id}},
        )

        if isinstance(ai_response, dict):
            answer = ai_response.get("answer", "")
        elif hasattr(ai_response, "content"):
            answer = ai_response.content
        else:
            answer = str(ai_response)

        answer = answer.strip()

        fallback_keywords = [
            "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì‹œê² ì–´ìš”?",
            "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        ]
        for phrase in fallback_keywords:
            if phrase in answer and len(answer.split(phrase)[0].strip()) > 50:
                answer = answer.split(phrase)[0].strip()
                break

        return answer

    except Exception as e:
        logging.error(f"âŒ AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
